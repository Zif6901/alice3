<script>
    // --- APP LOGIC ---
    const micBtn = document.getElementById('voice-trigger-btn');
    const muteBtn = document.getElementById('mute-btn');
    const transcriptBox = document.getElementById('live-transcript');
    const dfMessenger = document.querySelector('df-messenger');

    const WAKE_VARIANTS = ["alice", "alex", "allis", "alis", "allas", "ellis", "alas", "dallas"];
    const EXIT_COMMANDS = ["stop", "thank you", "thanks", "bye", "cancel"];
    
    // INCREASED from 400 to 1500 to stop cutting people off
    const STABILITY_DELAY = 1500; 
    const CONVERSATION_TIMEOUT_MS = 25000; 
    
    let isMicActive = false;
    let isProcessing = false; 
    let isMuted = false;
    let availableVoices = [];
    
    let commandTimer = null; 
    let conversationTimeoutTimer = null; 
    let safetyResetTimer = null; 
    let lastTranscript = "";

    const loadVoices = () => { availableVoices = window.speechSynthesis.getVoices(); };
    window.speechSynthesis.onvoiceschanged = loadVoices;
    loadVoices();

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (SpeechRecognition) {
      const recognition = new SpeechRecognition();
      recognition.continuous = true;  
      recognition.interimResults = true; 
      recognition.lang = 'en-US';

      micBtn.addEventListener('click', () => {
        // Create an empty utterance to unlock audio on mobile/safari
        const unlock = new SpeechSynthesisUtterance('');
        window.speechSynthesis.speak(unlock);
        
        if (isProcessing) {
            console.log("ðŸ‘† User Interrupted Audio!");
            window.speechSynthesis.cancel();
            resetProcessingState(); 
            return;
        }
        if (isMicActive) stopMic(); else startMic();
      });

      function startMic() {
          if (isProcessing) return; 
          updateButtonState("initializing");
          transcriptBox.style.display = "block";
          transcriptBox.innerText = "Starting Mic...";
          try { recognition.start(); } catch(e) {
              // If already started, just sync state
              if(e.message && e.message.includes('started')) onMicStartSuccess();
          }
      }

      recognition.onstart = () => { onMicStartSuccess(); };

      function onMicStartSuccess() {
          isMicActive = true;
          transcriptBox.innerText = "Listening...";
          updateButtonState("hearing"); 
          startConversationTimer();
      }

      function stopMic() {
          isMicActive = false;
          clearTimeout(conversationTimeoutTimer);
          clearTimeout(safetyResetTimer);
          clearTimeout(commandTimer); // Clear pending commands
          transcriptBox.style.display = "none";
          recognition.stop();
          window.speechSynthesis.cancel(); 
          updateButtonState("idle"); 
      }

      recognition.onresult = (event) => {
        if (isProcessing) return; 
        const results = event.results;
        const latestResult = results[results.length - 1];
        
        // Basic cleaning
        let transcript = latestResult[0].transcript.toLowerCase().trim();
        const cleanTranscript = transcript.replace(/[.,\/#!$%\^&\*;:{}=\-_`~()]/g,"");
        
        // Wake word detection (Optional: remove this block if you want it to hear everything)
        let command = transcript;
        const detectedVariant = WAKE_VARIANTS.find(w => cleanTranscript.includes(w));
        
        // Only split if we are sure we heard the wake word, otherwise keep full text
        if (detectedVariant) {
            const parts = transcript.split(new RegExp(detectedVariant, 'i'));
            command = parts[parts.length - 1].trim();
        }
        
        // Remove non-alphanumeric noise from start
        command = command.replace(/^[^a-zA-Z0-9]+/, '');

        // IGNORE very short blips (noise/breaths)
        if (command.length < 2) return;

        // Visual feedback
        const time = new Date().toLocaleTimeString().split(' ')[0];
        transcriptBox.innerText = `[${time}] Heard: "${command}"...`;

        // Reset the "Silence" timer
        startConversationTimer();

        // Check for exit commands immediately
        if (EXIT_COMMANDS.includes(command)) {
            endConversationMode("User ended session");
            return;
        }

        // DEBOUNCE LOGIC:
        // Every time the user speaks a new word, we clear the timer.
        // We only send the command if they stay silent for STABILITY_DELAY (1.5s).
        clearTimeout(commandTimer);
        commandTimer = setTimeout(() => { 
            executeCommand(command); 
        }, STABILITY_DELAY);
      };

      function executeCommand(text) {
          clearTimeout(commandTimer);
          if (isProcessing || text.length < 2) return; 
          
          isProcessing = true; 
          lastTranscript = "";
          updateButtonState("speaking");
          transcriptBox.innerText = `Sent: "${text}"`;
          console.log("SENDING:", text);
          
          recognition.stop(); 

          // Watchdog: If Dialogflow doesn't reply in 6s, reset.
          clearTimeout(safetyResetTimer);
          safetyResetTimer = setTimeout(() => {
              console.warn("Watchdog: No response. Resetting.");
              resetProcessingState(); 
          }, 6000);
          
          // Force open and send
          dfMessenger.setAttribute('expand', 'true');
          dfMessenger.renderCustomText(text, true);
          dfMessenger.sendRequest("query", text);
      }

      function startConversationTimer() {
          clearTimeout(conversationTimeoutTimer);
          conversationTimeoutTimer = setTimeout(() => { endConversationMode("Timeout"); }, CONVERSATION_TIMEOUT_MS);
      }

      function endConversationMode(reason) {
          console.log("Session Ended:", reason);
          stopMic(); 
      }

      function resetProcessingState() {
          isProcessing = false; 
          clearTimeout(safetyResetTimer);
          // If mic was active, restart it to loop
          if (isMicActive) {
             setTimeout(() => {
                 console.log("Restarting Mic...");
                 lastTranscript = ""; 
                 try { recognition.start(); } catch(e) {}
             }, 200);
          }
      }

      // Auto-restart handling
      recognition.onend = () => {
          if (isMicActive && !isProcessing) {
              // Mic died but shouldn't have? Restart it.
              setTimeout(() => { try { recognition.start(); } catch(e) {} }, 200); 
          } else if (!isMicActive) {
              updateButtonState("idle");
          }
      };

      recognition.onerror = (event) => {
          console.error("Mic Error:", event.error);
          if (event.error === 'not-allowed') stopMic();
          // Don't stop on 'no-speech', just let it loop
          if (event.error !== 'no-speech' && micBtn.classList.contains('initializing')) stopMic();
      };
    } else {
        alert("Browser not supported");
    }

    // --- UI HELPERS ---
    function updateButtonState(state) {
        micBtn.classList.remove('initializing', 'hearing', 'speaking');
        micBtn.style.backgroundColor = "#4285F4"; 
        if (state === "initializing") micBtn.classList.add('initializing');
        else if (state === "hearing") micBtn.classList.add('hearing');
        else if (state === "speaking") micBtn.classList.add('speaking');
    }

    // --- RESPONSE HANDLING ---
    window.addEventListener('df-response-received', (event) => {
        clearTimeout(safetyResetTimer); // Data received, cancel watchdog
        
        let botText = "";
        // 1. Try to find standard text response
        if (event.detail.messages) {
            const msg = event.detail.messages.find(m => m.type === 'text');
            if (msg) botText = msg.text;
        }
        // 2. Fallback to raw queryResult
        if (!botText && event.detail.raw && event.detail.raw.queryResult) {
            const qr = event.detail.raw.queryResult;
            if (qr.responseMessages) {
                const txt = qr.responseMessages.find(m => m.text);
                if (txt) botText = txt.text.text[0];
            } else if (qr.fulfillmentText) botText = qr.fulfillmentText;
        }

        // Speak if we found text
        if (botText && !isMuted) {
            transcriptBox.innerText = "Alice is speaking (Tap to Interrupt)...";
            speakText(botText);
        } else {
            // If no text (maybe just a chip or card), reset immediately
            resetProcessingState();
        }

        // Handle Audio Payload if present
        if (event.detail.raw && event.detail.raw.queryResult) {
            const msgs = event.detail.raw.queryResult.responseMessages || event.detail.raw.queryResult.fulfillmentMessages;
            if (msgs) {
                const audioMsg = msgs.find(m => m.payload && m.payload.audioUrl);
                if (audioMsg && !isMuted) playAudioFile(audioMsg.payload.audioUrl);
            }
        }
    });

    function speakText(text) {
        window.speechSynthesis.cancel();
        const utterance = new SpeechSynthesisUtterance(text);
        if (availableVoices.length === 0) availableVoices = window.speechSynthesis.getVoices();
        
        // Try to find a good Google US voice
        let v = availableVoices.find(v => v.name.includes("Google") && v.name.includes("US"));
        if (!v) v = availableVoices.find(v => v.lang.includes("en-"));
        if (v) utterance.voice = v;
        
        utterance.onend = () => { resetProcessingState(); };
        utterance.onerror = () => { resetProcessingState(); };
        window.speechSynthesis.speak(utterance);
    }

    function playAudioFile(url) {
        const audio = new Audio(url);
        audio.onended = resetProcessingState;
        audio.onerror = resetProcessingState;
        audio.play();
    }

    muteBtn.addEventListener('click', () => {
        isMuted = !isMuted;
        muteBtn.textContent = isMuted ? "ðŸ”‡" : "ðŸ”Š";
        muteBtn.classList.toggle('muted', isMuted);
        if (isMuted) window.speechSynthesis.cancel();
    });
  </script>

